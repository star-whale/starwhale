# ğŸ¦™ LLAMA2 meets Starwhale ğŸ‹

Thanks to [llama2](https://github.com/facebookresearch/llama) Project. This example demonstrates how to conduct a Starwhale Evaluation LLAMA2.

We use the llama2-7b-chat model for the example. The code base can work for the following scenarios:

1. Build Starwhale Model for base llama2 model.
2. Evaluate llama2 model.
3. Chat for llama2 model.

Current supports datasets:

- For evaluation:
  - mkqa
  - z_ben_common
  - vicuna
  - webqsp

## Build Starwhale Runtime

```bash
swcli runtime build --yaml runtime.yaml
```

example output:

```bash
$ swcli runtime build --yaml runtime.yaml

ğŸš§ start to build runtime bundle...
ğŸ‘· uri local/project/self/runtime/llama2/version/s4kudnma5ce7lbxudagr43rggrzigz3cfdrbmex4
ğŸ¦ runtime will ignore pypi editable package
ğŸ‘½ try to lock environment dependencies to /mnt/data/tianwei/code/starwhale/example/LLM/llama2 ...
ğŸ¦‹ lock dependencies at mode venv
ğŸ¼ install runtime.yaml dependencies @ /mnt/data/tianwei/code/starwhale/example/LLM/llama2/.starwhale/venv for lock...
ğŸ± use /mnt/data/tianwei/code/starwhale/example/LLM/llama2/.starwhale/venv/bin/python3 to freeze requirements...
ğŸ­ dump lock file: /mnt/data/tianwei/code/starwhale/example/LLM/llama2/.starwhale/lock/requirements-sw-lock.txt
ğŸ“ workdir: /home/liutianwei/.starwhale/self/workdir/runtime/llama2/s4/s4kudnma5ce7lbxudagr43rggrzigz3cfdrbmex4
ğŸ dump environment info...
ğŸ¥¯ dump dependencies info...
ğŸŒˆ runtime uses builtin docker image: docker-registry.starwhale.cn/star-whale/starwhale:0.5.6-cuda11.7  ğŸŒˆ
ğŸ¦‹ .swrt bundle:/home/liutianwei/.starwhale/self/runtime/llama2/s4/s4kudnma5ce7lbxudagr43rggrzigz3cfdrbmex4.swrt
  10 out of 10 steps finished â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00 0:00:00
```

## Build Starwhale Model for the llama2

```bash
swcli -vvv model build -m evaluation . --name llama2-7b-chat
swcli -vvv model build -m evaluation . --name llama2-7b-chat --runtime llama2
```

example output:

```bash
$ swcli -vvv model build -m evaluation . --name llama2-7b-chat
[2023-07-19 16:11:41.616914] ğŸ‘¾ verbosity: 3, log level: DEBUG
[2023-07-19 16:11:41.621373] ğŸš§ start to build model bundle...
[2023-07-19 16:11:41.623940] â“ |WARN| refine resource[ResourceType.model] llama2-7b-chat/version/f7sibscdttmw7wbeqnogakqljqtfjbyq65fmheav failed: Can not find the exact match item f7sibscdttmw7wbeqnogakqljqtfjbyq65fmheav, found: []
[2023-07-19 16:11:41.625017] ğŸ‘· uri local/project/self/model/llama2-7b-chat/version/f7sibscdttmw7wbeqnogakqljqtfjbyq65fmheav
[2023-07-19 16:11:41.627441] ğŸ”ˆ |DEBUG| ğŸ†• version f7sibscdttmw
[2023-07-19 16:11:41.628249] ğŸ“ workdir: /home/liutianwei/.starwhale/.tmp/tmpik4d1pp4
[2023-07-19 16:11:41.628835] ğŸ¦š copy source code files: . -> /home/liutianwei/.starwhale/.tmp/tmpik4d1pp4/src
[2023-07-19 16:11:41.629737] ğŸ”ˆ |DEBUG| copy dir: . -> /home/liutianwei/.starwhale/.tmp/tmpik4d1pp4/src, excludes: ['*/.git/*', '', '*/adapter-7b/checkpoint*', '*/adapter-7b/runs', '*/base-13b/*', '*/adapter-13b/*', '*/base-30b/*', '*/adapter-30b/*', '*/base-65b/*', '*/adapter-65b/*', '__pycache__/', '*.py', '*$py.class']
[2023-07-19 16:14:04.932350] ğŸ“ source code files size: 12.55GB
[2023-07-19 16:14:04.934443] ğŸ”ˆ |DEBUG| generating model serving config for local/project/self/model/llama2-7b-chat/version/f7sibscdttmw7wbeqnogakqljqtfjbyq65fmheav ...
[2023-07-19 16:14:20.768270] ğŸš€ generate jobs yaml from modules: ('evaluation',) , package rootdir: .
[2023-07-19 16:16:22.525950] ğŸ’¡ |INFO| ğŸ§º resource files size: 12.55GB
[2023-07-19 16:16:22.555182] ğŸ’¯ finish gen resource @ /home/liutianwei/.starwhale/self/model/llama2-7b-chat/f7/f7sibscdttmw7wbeqnogakqljqtfjbyq65fmheav.swmp
```

## Run Starwhale Model for evaluation in the Standalone instance

```bash
swcli -vvv model run -w . -m evaluation --handler evaluation:copilot_predict --dataset vicuna-mini
```

## Run Starwhale Model for chatting in the Standalone instance

```bash
swcli -vvv model run -w . -m evaluation --handler evaluation:chatbot
```

visit http://localhost:7860 for chatting.


